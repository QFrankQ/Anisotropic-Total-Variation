{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QFrankQ/Applied-Numerical-Optimization/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Background\n",
        "\n",
        "Noisy images are often an unwanted product of compression. Although they contain most of the information relevant to the original, desired image it can be difficult to isolate this information from the noise present in order to effectively remove it if you don‚Äôt have the isolated noise to begin with. So a central problem for image deconvolution is finding the noise present in an image, which involves defining what ‚Äúnoise‚Äù even is. \n",
        "\n",
        "Theoretically, a mathematical definition of the isolated noise present in an image could allow for an optimization problem under which that noise could be minimized. This noise is often difficult to distinguish from variation in pixel coloring that is inherently present in the picture e.g. edges, purposeful fading effects, etc. Still, a smoother color gradient across the image may make more sense to a human eye, but noise tends to refer to abnormal differences in adjacent pixel colorings. Thus, note that the goal of minimizing the noise in an image conflicts with another implicit goal, preserving the desired image‚Äôs information. Finding an optimal balance between these two objectives is key to obtaining the highest quality image possible. This means the objective we want to minimize is the following:\n",
        "\n",
        "\\begin{align*}\n",
        "\\underset{\\mathbf{X}}{\\text{minimize}} \\quad \\quad \\mathbf{D(x,y)} + \\tau \\mathbf{N(x)}\n",
        "\\end{align*}\n",
        "\n",
        "$\\mathbf{D(x,y)}$ represents the ‚Äúdistance‚Äù an image x is from y, the noisy image. Keeping this low helps preserve the content of the desired image. $\\mathbf{N(x)}$ is a metric of the noise present in the image x - note that y is not a parameter partly due to the aforementioned concept of there being no way to determine how much of the ‚Äúnoise‚Äù in y is actually meant to be in the original image vs. is that isolated noise we desire to remove. $\\tau$ is a basically a hyperparameter; it is a constant that represents the ratio of prioritization of the noise minimization to the distance between noisy image y and image x. The image x that minimizes this objective should theoretically be as close to the denoised image as we can possibly get assuming $\\tau$ was chosen effectively.\n",
        "\n",
        "There are many ways to separate the opposing objectives minimizing noise in an image and keeping the information present in the desired image. For the purposes of this project, we will specifically be looking at anisotropic total variation as a metric for the noise in an image. This metric prioritizes the differences in pixel value among adjacent pixels - vertically and horizontally that is, not diagonally. This leads to the minimization of the objective below. We will now expand and simplify this objective into one under which some separable problems emerge that can be tackled through Alternating Direction Method of Multiplier(ADMM) algorithm."
      ],
      "metadata": {
        "id": "I-gPb7DMknJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Anisotropic Total Variation**#\n",
        "\\begin{align*}\n",
        "\\underset{\\mathbf{X}}{\\text{minimize}} \\quad \\quad  \\frac{1}{2}\\| \\mathbf{x} - \\mathbf{y}\\|_F^2 + \\tau \\sum_{i,j}| \\nabla \\mathbf{X_{i,j}} |,\\\\\n",
        "\\text{where } | \\nabla \\mathbf{X_{i,j}} | = |\\mathbf{X}_{i+1,j} - \\mathbf{X}_{i,j}| + |\\mathbf{X}_{i,j+1} - \\mathbf{X}_{i,j}|\n",
        "\\end{align*}\n",
        "\n",
        "rewrite the problem as \n",
        "\\begin{align*}\n",
        "\\underset{\\mathbf{X}}{\\text{minimize}} \\quad \\quad  \\frac{1}{2}\\| \\mathbf{x} - \\mathbf{y}\\|_F^2 + \\tau \\sum_{i=1}^{N-1}\\sum_{j=1}^{N}| \\mathbf{X_{i+1,j} - \\mathbf{X_{i,j}}} | + \\tau \\sum_{i=1}^{N}\\sum_{j=1}^{N-1}| \\mathbf{X_{i,j+1} - \\mathbf{X_{i,j}}} |,\n",
        "\\end{align*}\n",
        "\n",
        "By Spliting, we get\n",
        "\\begin{align*}\n",
        "\\underset{\\mathbf{X,Z}}{\\text{minimize}} \\quad \\quad  \\frac{1}{2}\\| \\mathbf{x} - \\mathbf{y}\\|_F^2 + \\tau \\sum_{i=1}^{N-1}\\sum_{j=1}^{N}| \\mathbf{X_{i+1,j} - \\mathbf{X_{i,j}}} | + \\tau \\sum_{i=1}^{N}\\sum_{j=1}^{N-1}| \\mathbf{Z_{i,j+1} - \\mathbf{Z_{i,j}}} |,\n",
        "\\text{Subject to } \\mathbf{X = Z}\n",
        "\\end{align*}\n",
        "\n"
      ],
      "metadata": {
        "id": "zR34Bruoix-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Generate data. Randomly samle M * N normal distribution."
      ],
      "metadata": {
        "id": "r83T46FArc9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVaSW5vGirit"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "M = 30\n",
        "N = 30\n",
        "\n",
        "img = np.random.normal(0, 1, (M,N))\n",
        "noisy = img + np.random.normal(0,0.01,(M,N))\n",
        "#np.set_printoptions(suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create the convolution filter D for 1D total variation denoising."
      ],
      "metadata": {
        "id": "1fkQpVeX8Kg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def D(x):\n",
        "    # x is the input vector with length N\n",
        "    # compute res, an output vector with length N-1\n",
        "    \n",
        "    return np.array([x[i] - x[i-1] for i in range(1, x.shape[0])])\n",
        "  \n",
        "def DT(y):\n",
        "    # y is the input vector with length N-1\n",
        "    # compute res, an output vector with length N\n",
        "    return np.array([-y[i] if i == 0 else y[i-1] if i== y.shape[0] else y[i-1] - y[i] for i in range(y.shape[0]+1)])\n",
        "  \n",
        "def DDT(x):\n",
        "    # x is the input vector with length N-1\n",
        "    # compute res, an output vector with length N-1\n",
        "    \n",
        "    return np.array([2*x[i] - x[i+1] if i ==0 else 2*x[i] - x[i-1] if i == x.shape[0] -1 else -x[i-1] + 2*x[i] - x[i+1] for i in range(x.shape[0])])\n",
        "\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "CrHWcVyc8Jtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The objective, gradient and proximal operator\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Lagrangian is given by:} \\quad \\mathbf{L(X,Z,ùö™)} = \\frac{1}{2}\\| \\mathbf{X} - \\mathbf{Y}\\|_F^2 + \\frac{t}{2}\\|\\mathbf{X-Z+ùö™}\\|_F^2 + \\tau \\sum_{i=1}^{N-1}\\sum_{j=1}^{N}| \\mathbf{X_{i+1,j} - \\mathbf{X_{i,j}}} | + \\tau \\sum_{i=1}^{N}\\sum_{j=1}^{N-1}| \\mathbf{Z_{i,j+1} - \\mathbf{Z_{i,j}}} | \n",
        "\\end{align*}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rc58KYKCixbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The X subproblem would then be: \\\\\n",
        "\\begin{align*}\n",
        "\\quad\\underset{\\mathbf{X}}{\\text{minimize}}\\quad\\frac{1}{2}\\| \\mathbf{X} - \\mathbf{Y}\\|_F^2 + \\frac{t}{2}\\|\\mathbf{X-Z+ùö™}\\|_F^2 + \\tau \\sum_{i=1}^{N-1}\\sum_{j=1}^{N}| \\mathbf{X_{i+1,j} - \\mathbf{X_{i,j}}} |   \\\\\n",
        "\\end{align*}\n",
        "We then break down the first two terms and complete the squares:\n",
        "\\begin{align*}\n",
        "\\frac{1}{2}\\| \\mathbf{X} - \\mathbf{Y}\\|_F^2 + \\frac{t}{2}\\|\\mathbf{X-Z+ùö™}\\|_F^2 \\\\\n",
        "=\\frac{1}{2}(\\mathbf{X}^2 - 2\\mathbf{XY} + \\mathbf{Y}^2) + \\frac{t}{2}(\\mathbf{X}^2 - 2\\mathbf{X}(\\mathbf{Z} - ùö™)+(\\mathbf{Z}-ùö™)^2)\\\\\n",
        "=\\frac{1+t}{2}\\mathbf{X}^2+\\frac{1}{2}(-2\\mathbf{X}(\\mathbf{Y}+t(\\mathbf{Z}-ùö™)))+constant\\\\\n",
        "\\text{Divide the entire term by } \\frac{1+t}{2} \\text{ since the minimization problem doesn't change:}\\\\\n",
        "\\mathbf{X}^2-2\\mathbf{X}\\frac{\\mathbf{Y}+t(\\mathbf{Z}-ùö™)}{t+1} + constant\\\\\n",
        "\\text{Then complete the squares, we get}:\n",
        "\\|\\mathbf{X}-\\frac{\\mathbf{Y}+t(\\mathbf{Z}-ùö™)}{t+1}\\|_F^2\\\\\n",
        "\\end{align*}\n",
        "The new X-subproblem then becomes:\n",
        "\\begin{align*}\n",
        "\\quad\\underset{\\mathbf{X}}{\\text{minimize}} \\quad \\quad \\frac{1}{2}\\|\\mathbf{X}-\\frac{\\mathbf{Y}+t(\\mathbf{Z}-ùö™)}{t+1}\\|_F^2 +\\tau \\sum_{i=1}^{N-1}\\sum_{j=1}^{N}| \\mathbf{X_{i+1,j} - \\mathbf{X_{i,j}}} |\\\\\n",
        "\\text{The problem could then be decoupled into columns:}\\newline\n",
        "\\quad\\underset{\\mathbf{X}}{\\text{minimize}} \\quad \\quad \\sum_{j=1}^{N} \\left(\\quad \\frac{1}{2}\\|\\mathbf{X_j}-\\frac{\\mathbf{Y_j}+t(\\mathbf{Z_j}-ùö™_\\mathbf{j})}{t+1}\\|_2^2+\\tau \\sum_{i=1}^{N-1}| \\mathbf{X_{i+1,j} - \\mathbf{X_{i,j}}} |\\right)\\\\\n",
        "\\text{Then perform 1D total Variation on each columns, which turns the problem into:}\\\\\n",
        "\\underset{\\mathbf{X}}{\\text{minimize}} \\quad \\sum_{j = 1}^{N} \\quad \\underset{\\boldsymbol{\\gamma} \\in \\mathbb{R}^{N-1}}{\\text{minimize}}  \\quad \\quad & \\frac{1}{2}\\| \\mathbf{y} - \\mathbf{D}^T \\boldsymbol{\\gamma}\\|_2^2, \\\\\n",
        "\\text{subject to}  \\quad \\quad & \\| \\boldsymbol{\\gamma}\\|_\\infty \\leq \\lambda\n",
        "\\end{align*}\n",
        "where $y = \\frac{\\mathbf{Y_j}+t(\\mathbf{Z_j}-ùö™_\\mathbf{j})}{t+1}$\n",
        "\n",
        "The Z subproblem would then be:\n",
        "\\begin{align*}\n",
        "\\quad\\underset{\\mathbf{Z}}{\\text{minimize}} \\quad \\quad \\frac{t}{2}\\|\\mathbf{X-Z+ùö™}\\|_F^2 + \\tau \\sum_{i=1}^{N}\\sum_{j=1}^{N-1}| \\mathbf{Z_{i,j+1} - \\mathbf{Z_{i,j}}} |\n",
        "\\end{align*}\n",
        "The Z subproblem then becomes:\n",
        "\\begin{align*}\n",
        "\\text{The problem could then be decoupled into rows:}\\\\\n",
        "\\quad\\underset{\\mathbf{Z}}{\\text{minimize}} \\quad \\quad \\sum_{i=1}^{N} \\left(\\quad \\frac{1}{2}\\|\\mathbf{Z_i}- (\\mathbf{X_i}+ùö™_\\mathbf{i})\\|_2^2+ \\tau \\sum_{j=1}^{N-1}| \\mathbf{Z_{i,j+1} - \\mathbf{Z_{i,j}}} |\\right)\\\\\n",
        "\\text{Then perform 1D total Variation on each rows, which turns the problem into:}\\\\\n",
        "\\underset{\\mathbf{Z}}{\\text{minimize}} \\quad \\sum_{j = 1}^{N} \\quad \\underset{\\boldsymbol{\\gamma} \\in \\mathbb{R}^{N-1}}{\\text{minimize}}  \\quad \\quad & \\frac{1}{2}\\| \\mathbf{y} - \\mathbf{D}^T \\boldsymbol{\\gamma}\\|_2^2, \\\\\n",
        "\\text{subject to}  \\quad \\quad & \\| \\boldsymbol{\\gamma}\\|_\\infty \\leq \\lambda\n",
        "\\end{align*}\n",
        "where $y = \\mathbf{X_i}+ùö™_\\mathbf{i}$"
      ],
      "metadata": {
        "id": "H-LP0plnZIZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda gamma, y: 1/2 *pow(np.linalg.norm(y - DT(gamma)), 2) \n",
        "fp = lambda gamma, y: DDT(gamma) - D(y) \n",
        "prox = lambda z, lam: np.maximum(np.minimum(z, lam), -lam)\n",
        "dual_to_primal = lambda gamma, y: y - DT(gamma) #convert the dual variable back to primal\n",
        "X_sub_to_y = lambda y, z, gamma, t: y/(t+1) + t/(t+1)*(z-gamma)\n",
        "Z_sub_to_y = lambda x, gamma: x + gamma\n",
        "\n",
        "def X_prox_gradient(y, z, gamma, lam, t):\n",
        "  ss = 1/4\n",
        "  maxit = 100\n",
        "  tol = 1e-3\n",
        "  change = math.inf\n",
        "  it = 0\n",
        "  dual_prob_gamma = np.zeros(y.shape[0]-1)#The small gamma in 1D Total Variation dual problem constraint, not to be confused with ADMM Gamma[i]    #potential bug\n",
        "  dual_prob_y = X_sub_to_y(y,z,gamma, t)#The y in dual problem of X subproblem over rows/columns, not to be confused with Y[i]\n",
        "  while it < maxit and change > tol:\n",
        "    z = dual_prob_gamma - ss * fp(dual_prob_gamma, dual_prob_y)\n",
        "    new_gamma = prox(z, lam)\n",
        "    old_obj = f(dual_prob_gamma, y)\n",
        "    change = abs(f(new_gamma, y) - old_obj)/ abs(old_obj)\n",
        "    dual_prob_gamma = new_gamma\n",
        "    #print(f\"it{it}: {}\")\n",
        "    it += 1  \n",
        "  #print(x[:5])\n",
        "  return dual_to_primal(dual_prob_gamma, dual_prob_y)\n",
        "\n",
        "def Z_prox_gradient(y, x, gamma, lam, t):\n",
        "  ss = 1/4\n",
        "  maxit = 100\n",
        "  tol = 1e-3\n",
        "  change = math.inf\n",
        "  it = 0\n",
        "  dual_prob_gamma = np.zeros(y.shape[0]-1)#The small gamma in 1D Total Variation dual problem constraint, not to be confused with ADMM Gamma[i]    #potential bug\n",
        "  dual_prob_y = Z_sub_to_y(x, gamma)\n",
        "  while it < maxit and change > tol:\n",
        "    z = dual_prob_gamma - ss * fp(dual_prob_gamma, dual_prob_y)\n",
        "    new_gamma = prox(z, lam)\n",
        "    old_obj = f(dual_prob_gamma, y)\n",
        "    change = abs(f(new_gamma, y) - old_obj)/ abs(old_obj)\n",
        "    dual_prob_gamma = new_gamma\n",
        "    it += 1\n",
        "    #print(f\"it:{it} {change}\")\n",
        "  return dual_to_primal(dual_prob_gamma, dual_prob_y) #\n",
        "\n",
        "# 1D total variation on the columns\n",
        "# Y[:,i] is the ith column of Y; take the transpose at the end to convert back to actual new X\n",
        "solve_X = lambda Y, Z, Gamma, lam, t: np.array([X_prox_gradient(Y[:,i], Z[:,i], Gamma[:,i], lam, t) for i in range(Y.shape[1])]).T\n",
        "\n",
        "# 1D total variation on the rows\n",
        "solve_Z = lambda Y, X, Gamma, lam, t: np.array([Z_prox_gradient(Y[i], X[i],Gamma[i], lam, t) for i in range(Y.shape[0])])\n"
      ],
      "metadata": {
        "id": "O4VC54lrEkgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ADMM function "
      ],
      "metadata": {
        "id": "APzTIsl9ABTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ADMM(Y, lam, maxit, tol):\n",
        "  X = np.zeros(Y.shape)\n",
        "  Z = np.zeros(Y.shape)\n",
        "  Gamma = np.zeros(Y.shape)\n",
        "  it = 0\n",
        "  X_change = math.inf\n",
        "  Z_change = math.inf\n",
        "  t = 1\n",
        "  while it < maxit and (X_change > tol or Z_change > tol):\n",
        "    #X subproblem: 1D Total Variation Across the rows\n",
        "\n",
        "    new_X = solve_X(Y, Z, Gamma, lam, t)\n",
        "\n",
        "\n",
        "    #Z subproblem: 1D Total Variation Across the columns\n",
        "    new_Z = solve_Z(Y, new_X, Gamma, lam, t)\n",
        "\n",
        "    #update Gamma\n",
        "    Gamma = Gamma + new_X - new_Z\n",
        "    print(f\" max entry in new_X: {np.amax(new_X)}\")\n",
        "    print(f\" max entry in new_Z: {np.amax(new_Z)}\")\n",
        "    it += 1\n",
        "    X_change = np.linalg.norm((new_X - X), ord = 'fro')\n",
        "    Z_change = np.linalg.norm((new_Z - Z), ord = 'fro')\n",
        "    print(f\"Xchange: {X_change}\")\n",
        "    print(f\"Zchange: {Z_change}\")\n",
        "    X = new_X\n",
        "    Z = new_Z\n",
        "    print(it)\n",
        "    #print(f\"x[0,4]: {X[0,4]}\")\n",
        "  return X, it"
      ],
      "metadata": {
        "id": "LqS3GEFJEkmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing on randomly generated 30 by 30 matrix"
      ],
      "metadata": {
        "id": "SUcD5r56Xglx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxit = 10000\n",
        "tol = 1e-15\n",
        "result = ADMM(noisy, 0.1, maxit, tol)"
      ],
      "metadata": {
        "id": "QxgTLdHyYPQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputing 2D image.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zPuEk5YtXV9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def open_as_nparray(filename, desired_width = None, desired_height = None):\n",
        "  im = Image.open(filename).convert('L')\n",
        "  #defaults to original size\n",
        "  if desired_width is not None and desired_height is not None:\n",
        "    im = im.resize((desired_width, desired_height))\n",
        "  return np.asarray(im)/255\n",
        "\n",
        "def add_noise(original_image, noise_amplitude):\n",
        "  noisy = original_image + np.random.normal(0,noise_amplitude,original_image.shape)\n",
        "  return noisy\n",
        "\n",
        "def diff_heatmap(original_image, recovered_image):\n",
        "  diff = original_image - recovered_image\n",
        "  return np.absolute(diff)\n",
        "\n",
        "def comparison_plot(original_image, size, noisy_image, recovered_image, tau, iters, difference_heatmap, noise):\n",
        "  fig = plt.figure(figsize = (20, 14))\n",
        "  rows = 1\n",
        "  columns = 4\n",
        "  \n",
        "  #original image\n",
        "  fig.add_subplot(rows, columns, 1)\n",
        "  plt.imshow(original_image, cmap = 'gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Original Image, size = \" + str(size[0]) + \"x\" + str(size[1]))\n",
        "  \n",
        "  #noisy image\n",
        "  fig.add_subplot(rows, columns, 2)\n",
        "  plt.imshow(noisy_image, cmap = 'gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Noisy Image, noise = \" + str(noise))\n",
        "  \n",
        "  #recovered image\n",
        "  fig.add_subplot(rows, columns, 3)\n",
        "  plt.imshow(recovered_image, cmap = 'gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Recovered Image, tau = \" + str(tau))\n",
        "  \n",
        "  #difference heatmap\n",
        "  fig.add_subplot(rows, columns, 4)\n",
        "  plt.imshow(difference_heatmap, cmap = 'hot')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Heatmap of Differences, iterations to converge = \" + str(iters))\n",
        "\n",
        "def plot_from_file(filename, noise, tau, desired_width = None, desired_height = None):\n",
        "  #defaults to original size\n",
        "  original = open_as_nparray(filename, desired_width = desired_width, desired_height = desired_height)\n",
        "  noisy = add_noise(original, noise)\n",
        "  recovered, iters = ADMM(noisy, tau, 5000, float('1e-12'))\n",
        "  \n",
        "  difference = diff_heatmap(original, recovered)\n",
        "  comparison_plot(original, (desired_width, desired_height), noisy, recovered, tau, iters, difference, noise)\n",
        "\n",
        "def plot_multiple_from_file(filenames, sizes = None, noises = 0.05, taus = 0.01):\n",
        "  #sizes can be left blank or set to none to use original sizes\n",
        "  #to use original size for individual images, set their entry in sizes to None\n",
        "\n",
        "  #similarly, noises can be a single value, an array of values, or left blank\n",
        "  if isinstance(noises, (int, float, complex)):\n",
        "    noise_array = np.full(len(filenames), noises)\n",
        "  else:\n",
        "    noise_array = noises\n",
        "\n",
        "  if isinstance(taus, (int, float, complex)):\n",
        "    tau_array = np.full(len(filenames), taus)\n",
        "  else:\n",
        "    tau_array = taus\n",
        "\n",
        "  if sizes is None:\n",
        "    for (filename, noise, tau) in zip(filenames, noise_array, tau_array):\n",
        "      plot_from_file(filename, noise, tau)\n",
        "  \n",
        "  else:\n",
        "    l = len(filenames)\n",
        "    if l != len(sizes):\n",
        "      raise Exception(\"filenames, sizes, and noises must have the same size! sizes and noises can be left blank\")\n",
        "\n",
        "    for (filename, size, noise, tau) in zip(filenames, sizes, noise_array, tau_array):\n",
        "      if size is None:\n",
        "        plot_from_file(filename, noise, tau)\n",
        "      else:\n",
        "        plot_from_file(filename, noise, tau, desired_width = size[0], desired_height = size[1])"
      ],
      "metadata": {
        "id": "d-_J5cvYmdUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = ['smile.jpeg']\n",
        "sizes = [(50, 50)]\n",
        "noises = 0.05\n",
        "taus = 0.01\n",
        "plot_multiple_from_file(filenames, sizes = sizes, noises = noises, taus = taus)"
      ],
      "metadata": {
        "id": "y4sgo8vVd18o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = ['smile.jpeg', 'smile.jpeg', 'smile.jpeg', 'smile.jpeg', 'smile.jpeg', 'smile.jpeg', 'smile.jpeg']\n",
        "#you need to upload your own files because it doesn't save across sessions\n",
        "sizes = [(20, 20), (50, 50), (100, 100), (50, 50), (50, 50), (50, 50), (50, 50)]\n",
        "noises = [0.05, 0.05, 0.05, 0.01, 0.01, 0.1, 0.1]\n",
        "taus = [0.01, 0.01, 0.01, 0.005, 0.02, 0.005, 0.02]\n",
        "#sizes can be left blank to use original size, or an index can be set to none to use original size\n",
        "#noises can be left blank to default to 0.05, a single number for the same noise over all images,\n",
        "#or an arrray of noises if you want different noise on images\n",
        "#taus is the same as noises\n",
        "plot_multiple_from_file(filenames, sizes = sizes, noises = noises, taus = taus)"
      ],
      "metadata": {
        "id": "AlqnYRGKmf0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing an optimal value of the hyperparameter is unique to each specific image and the properties such as level of noise inherent to the image vs. level of noise added on that needs to be isolated and removed. For these images, we found that $\\tau \\in [0.01,0.1]$ worked well, although higher values for  $\\tau$ seemed to increase runtimes in some cases. Lower values of $\\tau$ failed to prioritize denoising enough, so noisy images were returned. Higher values of $\\tau$ also tended to start taking out crucial details like borders and outlines which makes sense as \"smoothing\" was overprioritized."
      ],
      "metadata": {
        "id": "5L_pXNSVmrK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the image returned from running ADMM to minimize the objective relevant to denoising an image using anisotropic total variation as the metric for noise present is clearly less noisy or fuzzy than the original noisy image we started with. However, it‚Äôs not perfect - much of this can be explained by drawbacks to the choice of metric used. Notice how the algorithm does well to remove noise around areas of the image where the pixel color gradient isn‚Äôt very high. This is because anisotropic total variation due to noise versus just due to differences in the actual desired image in these regions is clearly distinguishable. However, this is not true for areas of the image where color is supposed to be changed abruptly such as for borders that outline facial features. Notice the eyebrows, thumb outline, jawline, eyes, etc. are still noisy in the produced image. Metrics of noise can‚Äôt perfectly distinguish these borders from noise added; however, some metrics will do better e.g. by giving larger differences in adjacent pixel values less priority in the objective so that borders are more often kept. \n"
      ],
      "metadata": {
        "id": "oBINAmUslXM5"
      }
    }
  ]
}